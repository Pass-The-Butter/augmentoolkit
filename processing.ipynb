{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Augmentoolkit\n",
    "\n",
    "This notebook is where you generate all your data.\n",
    "\n",
    "Augmentoolkit is meant to allow instruct-tuned models to learn from books, even using themselves to generate new data through a sort-of bootstrapping method. It is meant to stop model creators from having to work as data annotators, and not actual model trainers. It is meant to allow anyone to make their own high-quality dataset with thousands of entries using cheap Open-Source APIs.\n",
    "\n",
    "## Quickstart:\n",
    "\n",
    "- Get this notebook and the other repo code onto a machine with an internet connection\n",
    "- Paste your API key, favorite model name, and the endpoint URL of your preferred AI service, into the relevant constants located in the first code cell. Recommendation: [Together.ai with Hermes Mixtral works really nicely](https://api.together.xyz/playground/chat/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO).\n",
    "- Run all the cells below and watch as the notebook generates questions, answers, and conversations based on Principles of Chemistry and Simple Sabotage.\n",
    "\n",
    "If you want to add your own texts, follow the instructions in list item #1 above.\n",
    "\n",
    "### Note: this notebook makes roughly 1/3 characters generated to be **mildly NSFW** by default. You will need to modify the character personality code in `./generation_functions/special_instructions.py` or use \"Assistant mode\" if you want something cleaner.\n",
    "\n",
    "## Customization:\n",
    "### Here are some ways you can adapt this notebook to your use case, along with a brief description of how to do so, arranged in increasing order of difficulty (this information is also available in the README):\n",
    "1. ***Change the source texts used to generate training data.*** You can do this in the cell right below this one. **IMPORTANT** the filenames of these should be formatted in a specific way, since the filenames are used as part of the prompts and in at least one regex. You need to have them be like: `[textname], by authorname`. You can also include the publication date after the author name if you want, but note that this will tend to bias most of the characters to live in the era of the textbook, which may or may not be what you want.\n",
    "\n",
    "2. ***Change the personalities of the characters generated.*** Currently, when generating characters for the multiturn conversation step, three randomly-selected traits are appended to the \"special instructions\" set of the prompt to constrain what kind of character is generated by the model. Depending on what kind of model you want to make, or even just if your preferences vary, then you will probably want to modify this a bit. You can do so in `./generation_functions/special_instructions.py`. A more in-depth description of the trait-axis system that I (over)thought up is available in the comments of that file.\n",
    "\n",
    "3. ***Change the constants.*** There are a few constant values in this notebook, and in `./generation_functions/constant_values.py`. These constants are tested, but if your use case requires special settings (e.g., you want to make conversations from more permutations of existing questions; or you think the character counts for the \"duplicate question/answer\" validation functions are too restrictive) then feel free to change the related setting. The most intuitive and least-likely-to-break-anything settings to change are rearrangements_to_take and double_check_counter. Beyond that... you'll need to figure out what the function does before changing it if you expect it to run.\n",
    "\n",
    "4. ***Assistant Mode*** Technically this could be considered part of 3), but it's different enough that I feel it warrants separate explanation. By default, the notebook is configured to produce RP-style data; \"Assistant mode\" is something you can toggle in the settings cell immediately below this one, which skips character and scenario generation and answers every question in a chat between a user and a helpful AI assistant (with no personality). This can be handled by smaller models, so if your budget or time are very limited, or you are using this for a more professional use case, feel free to turn this on.\n",
    "\n",
    "5. ***Change the model.*** This is as simple as switching the LOGICAL_MODEL value out for another one, but your mileage may vary significantly. My personal recommendation is to use [Hermes Mixtral DPO](https://api.together.xyz/playground/chat/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) for both models. You need at least 12k context on your model, and APIs typically don't allow RoPE scaling, so you're probably limited to MistralAI models here (or, heavens forbid, OpenAI. But GPT-4.5 + Augmentoolkit will BANKRUPT you fast, so be wary).\n",
    "\n",
    "6. ***Change the examples.*** If you change the examples you can completely overhaul what this notebook does, but this requires a lot of prompting skill and possibly huge amounts of time to get it working again (source: most of my last three months were spent prompting, and most of this prompting was spent on the examples). Unless you want to convert this notebook from question-and-answer generation to some completely other task, I'd recommend changing only the conversation generation prompts -- they're a bit less finnicky, and if you just want to change the kind of characters generated (maybe you want a different writing style) that's where you'd find the differences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "You will want to turn off USE_SUBSET if you are doing a proper run over an entire text. It's on by default so you can iterate faster in the preparatory stages of dataset generation (and so that you can see the magic happen faster when you first use Augmentoolkit :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE NOTEBOOK SETTINGS AND CONSTANTS (some script file constants are in generation_functions/constants.py)\n",
    "\n",
    "# Put your desired quant of your desired model in the relevant directories\n",
    "\n",
    "\n",
    "# \"airoboros-l2-70b-3.1.2.Q4_K_M.gguf\" <- recommended for the large logical model\n",
    "# \"flatorcamaid-13b-v0.2.Q8_0.gguf\" <- recommended for the normal logical model\n",
    "# A6000s on Vast.ai are a good choice for running this notebook\n",
    "\n",
    "LOGICAL_MODEL = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"#\"TheBloke/FlatOrcamaid-13B-v0.2-GPTQ\"  # model used for decision-making and base question generation (should be \"smart\")\n",
    "\n",
    "LARGE_LOGICAL_MODEL = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"#\"TheBloke/Airoboros-L2-70B-3.1.2-GPTQ\"\n",
    "\n",
    "ASSISTANT_MODE = False  # change to true if you want all conversations to be with an \"AI language model\" and not characters. Useful for more professional use cases.\n",
    "\n",
    "DOUBLE_CHECK_COUNTER = 3  # Set to 1 to check outputs only once; set to 2 to check twice; set to 3 to check thrice, etc. Set to 0 to break everything in vet_question_loop() and elsewhere. Set to -1 and cause the universe to implode?\n",
    "\n",
    "USE_SUBSET = False # Set to True if you want to use only a small subset of the text, to test whether it plays nicely with the current setup of the notebook\n",
    "\n",
    "REARRANGEMENTS_TO_TAKE = 3  # How many of the possible permutations of tuples in a group to take and make multiturn convs out of. Adjust higher to get more data out of less text, but it might be a bit repetitive. NOTE your eval loss will be basically worthless if you aren't careful with how you shuffle your dataset when you're about to train.\n",
    "\n",
    "USE_FILENAMES = False # Turn on if you want the model to use the names of your files as additional context (this is what original Augmentoolkit does). Useful if you have a small number of large input files grouped by subject matter, IE books. Turn off if you have a large number of files with meaningless names.\n",
    "\n",
    "CONCURRENCY_LIMIT = 90  # Adjust this number based on the rate limit constraints of your api\n",
    "\n",
    "API_KEY = \"YOUR KEY HERE\"\n",
    "\n",
    "BASE_URL = \"https://api.together.xyz\" # Augmentoolkit-API should also be compatible with any other API provider that accepts OAI-style requests\n",
    "\n",
    "source_texts = [ # add your texts here\n",
    "    \"text1.txt\",\n",
    "    \"text2.txt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below: Defines and imports functions that you will probably use no matter what cells in the notebook you choose to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "# This is in no way best practices, but all my prompts being searchable and separate files is a good way to make my life easier.\n",
    "import pkgutil\n",
    "import importlib\n",
    "import sys\n",
    "from tqdm import asyncio as tqdmasyncio\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# Set up rate-limit-conscious functions\n",
    "semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "\n",
    "async def run_task_with_limit(task):\n",
    "    async with semaphore:\n",
    "        # Run your task here\n",
    "        return await task\n",
    "\n",
    "\n",
    "# We have to define this up here so that two-step generation works, you'll see later.\n",
    "multi_turn_convs_info_dir = \"./multi_turn_convs_info\"  # we generate all the information fed to the multiturn prompt, and generate the actual multiturn prompt, separately; since every step but the last is capable of being done by a 13b\n",
    "\n",
    "sys.path.append(\"./generation_functions\")\n",
    "sys.path.append(\"./control_flow_functions\")\n",
    "\n",
    "import augmentoolkit.generation_functions as generation_functions  # This is the package directory\n",
    "from augmentoolkit.control_flow_functions import control_flow_functions\n",
    "\n",
    "# First, import all modules so they can be reloaded\n",
    "for _, module_name, _ in pkgutil.iter_modules(\n",
    "    generation_functions.__path__, generation_functions.__name__ + \".\"\n",
    "):\n",
    "    importlib.import_module(module_name)\n",
    "\n",
    "# Now, reload each module and import all callable attributes\n",
    "for _, module_name, _ in pkgutil.iter_modules(\n",
    "    generation_functions.__path__, generation_functions.__name__ + \".\"\n",
    "):\n",
    "    # Reload the module\n",
    "    module = importlib.reload(sys.modules[module_name])\n",
    "    # Iterate through each attribute in the reloaded module\n",
    "    for attribute_name in dir(module):\n",
    "        # Retrieve the attribute\n",
    "        attribute = getattr(module, attribute_name)\n",
    "        if callable(attribute):\n",
    "            # If it's callable, it's a function or class, so you set it in the globals dictionary\n",
    "            globals()[attribute_name] = attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API Client\n",
    "engine_wrapper = EngineWrapper(model=LOGICAL_MODEL,api_key=API_KEY,base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_files(start=3, end=500, dir_path='./'):\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    for i in range(start, end + 1):\n",
    "        filename = f\"test{i}.txt\"\n",
    "        source_texts.append(filename)\n",
    "        filepath = os.path.join(dir_path, filename)\n",
    "        \n",
    "        # Generate random short content\n",
    "        content_length = random.randint(5, 100)  # Adjust length as needed\n",
    "        content = ''.join(random.choices(string.ascii_letters + string.digits, k=content_length))\n",
    "        \n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(content)\n",
    "\n",
    "# Call the function to generate the files in the current directory\n",
    "generate_random_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/evan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Processing text1.txt: 100%|██████████| 1/1 [00:00<00:00, 1256.16it/s]\n",
      "Processing text2.txt: 100%|██████████| 1/1 [00:00<00:00, 2794.34it/s]\n",
      "Processing test3.txt: 100%|██████████| 1/1 [00:00<00:00, 2083.61it/s]\n",
      "Processing test4.txt: 100%|██████████| 1/1 [00:00<00:00, 4332.96it/s]\n",
      "Processing test5.txt: 100%|██████████| 1/1 [00:00<00:00, 4405.78it/s]\n",
      "Processing test6.txt: 100%|██████████| 1/1 [00:00<00:00, 4112.06it/s]\n",
      "Processing test7.txt: 100%|██████████| 1/1 [00:00<00:00, 4401.16it/s]\n",
      "Processing test8.txt: 100%|██████████| 1/1 [00:00<00:00, 3986.98it/s]\n",
      "Processing test9.txt: 100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]\n",
      "Processing test10.txt: 100%|██████████| 1/1 [00:00<00:00, 4198.50it/s]\n",
      "Processing test11.txt: 100%|██████████| 1/1 [00:00<00:00, 3509.88it/s]\n",
      "Processing test12.txt: 100%|██████████| 1/1 [00:00<00:00, 3013.15it/s]\n",
      "Processing test13.txt: 100%|██████████| 1/1 [00:00<00:00, 3192.01it/s]\n",
      "Processing test14.txt: 100%|██████████| 1/1 [00:00<00:00, 2779.53it/s]\n",
      "Processing test15.txt: 100%|██████████| 1/1 [00:00<00:00, 2555.94it/s]\n",
      "Processing test16.txt: 100%|██████████| 1/1 [00:00<00:00, 3223.91it/s]\n",
      "Processing test17.txt: 100%|██████████| 1/1 [00:00<00:00, 4969.55it/s]\n",
      "Processing test18.txt: 100%|██████████| 1/1 [00:00<00:00, 3095.43it/s]\n",
      "Processing test19.txt: 100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]\n",
      "Processing test20.txt: 100%|██████████| 1/1 [00:00<00:00, 2824.45it/s]\n",
      "Processing test21.txt: 100%|██████████| 1/1 [00:00<00:00, 2582.70it/s]\n",
      "Processing test22.txt: 100%|██████████| 1/1 [00:00<00:00, 3086.32it/s]\n",
      "Processing test23.txt: 100%|██████████| 1/1 [00:00<00:00, 3310.42it/s]\n",
      "Processing test24.txt: 100%|██████████| 1/1 [00:00<00:00, 5133.79it/s]\n",
      "Processing test25.txt: 100%|██████████| 1/1 [00:00<00:00, 3536.51it/s]\n",
      "Processing test26.txt: 100%|██████████| 1/1 [00:00<00:00, 5907.47it/s]\n",
      "Processing test27.txt: 100%|██████████| 1/1 [00:00<00:00, 4258.18it/s]\n",
      "Processing test28.txt: 100%|██████████| 1/1 [00:00<00:00, 3663.15it/s]\n",
      "Processing test29.txt: 100%|██████████| 1/1 [00:00<00:00, 2198.27it/s]\n",
      "Processing test30.txt: 100%|██████████| 1/1 [00:00<00:00, 4675.92it/s]\n",
      "Processing test31.txt: 100%|██████████| 1/1 [00:00<00:00, 3938.31it/s]\n",
      "Processing test32.txt: 100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "Processing test33.txt: 100%|██████████| 1/1 [00:00<00:00, 4519.72it/s]\n",
      "Processing test34.txt: 100%|██████████| 1/1 [00:00<00:00, 3802.63it/s]\n",
      "Processing test35.txt: 100%|██████████| 1/1 [00:00<00:00, 3701.95it/s]\n",
      "Processing test36.txt: 100%|██████████| 1/1 [00:00<00:00, 2951.66it/s]\n",
      "Processing test37.txt: 100%|██████████| 1/1 [00:00<00:00, 3572.66it/s]\n",
      "Processing test38.txt: 100%|██████████| 1/1 [00:00<00:00, 3844.46it/s]\n",
      "Processing test39.txt: 100%|██████████| 1/1 [00:00<00:00, 3004.52it/s]\n",
      "Processing test40.txt: 100%|██████████| 1/1 [00:00<00:00, 5077.85it/s]\n",
      "Processing test41.txt: 100%|██████████| 1/1 [00:00<00:00, 2545.09it/s]\n",
      "Processing test42.txt: 100%|██████████| 1/1 [00:00<00:00, 3938.31it/s]\n",
      "Processing test43.txt: 100%|██████████| 1/1 [00:00<00:00, 3597.17it/s]\n",
      "Processing test44.txt: 100%|██████████| 1/1 [00:00<00:00, 4152.78it/s]\n",
      "Processing test45.txt: 100%|██████████| 1/1 [00:00<00:00, 3066.01it/s]\n",
      "Processing test46.txt: 100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n",
      "Processing test47.txt: 100%|██████████| 1/1 [00:00<00:00, 3634.58it/s]\n",
      "Processing test48.txt: 100%|██████████| 1/1 [00:00<00:00, 3509.88it/s]\n",
      "Processing test49.txt: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Processing test50.txt: 100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "Processing test51.txt: 100%|██████████| 1/1 [00:00<00:00, 5071.71it/s]\n",
      "Processing test52.txt: 100%|██████████| 1/1 [00:00<00:00, 3968.12it/s]\n",
      "Processing test53.txt: 100%|██████████| 1/1 [00:00<00:00, 4981.36it/s]\n",
      "Processing test54.txt: 100%|██████████| 1/1 [00:00<00:00, 4975.45it/s]\n",
      "Processing test55.txt: 100%|██████████| 1/1 [00:00<00:00, 4911.36it/s]\n",
      "Processing test56.txt: 100%|██████████| 1/1 [00:00<00:00, 5096.36it/s]\n",
      "Processing test57.txt: 100%|██████████| 1/1 [00:00<00:00, 5005.14it/s]\n",
      "Processing test58.txt: 100%|██████████| 1/1 [00:00<00:00, 2966.27it/s]\n",
      "Processing test59.txt: 100%|██████████| 1/1 [00:00<00:00, 3653.57it/s]\n",
      "Processing test60.txt: 100%|██████████| 1/1 [00:00<00:00, 3279.36it/s]\n",
      "Processing test61.txt: 100%|██████████| 1/1 [00:00<00:00, 2666.44it/s]\n",
      "Processing test62.txt: 100%|██████████| 1/1 [00:00<00:00, 2033.11it/s]\n",
      "Processing test63.txt: 100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]\n",
      "Processing test64.txt: 100%|██████████| 1/1 [00:00<00:00, 2501.08it/s]\n",
      "Processing test65.txt: 100%|██████████| 1/1 [00:00<00:00, 3858.61it/s]\n",
      "Processing test66.txt: 100%|██████████| 1/1 [00:00<00:00, 8081.51it/s]\n",
      "Processing test67.txt: 100%|██████████| 1/1 [00:00<00:00, 5722.11it/s]\n",
      "Processing test68.txt: 100%|██████████| 1/1 [00:00<00:00, 2716.52it/s]\n",
      "Processing test69.txt: 100%|██████████| 1/1 [00:00<00:00, 3830.41it/s]\n",
      "Processing test70.txt: 100%|██████████| 1/1 [00:00<00:00, 4485.89it/s]\n",
      "Processing test71.txt: 100%|██████████| 1/1 [00:00<00:00, 6105.25it/s]\n",
      "Processing test72.txt: 100%|██████████| 1/1 [00:00<00:00, 6584.46it/s]\n",
      "Processing test73.txt: 100%|██████████| 1/1 [00:00<00:00, 5289.16it/s]\n",
      "Processing test74.txt: 100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "Processing test75.txt: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "Processing test76.txt: 100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "Processing test77.txt: 100%|██████████| 1/1 [00:00<00:00, 8144.28it/s]\n",
      "Processing test78.txt: 100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]\n",
      "Processing test79.txt: 100%|██████████| 1/1 [00:00<00:00, 4152.78it/s]\n",
      "Processing test80.txt: 100%|██████████| 1/1 [00:00<00:00, 5548.02it/s]\n",
      "Processing test81.txt: 100%|██████████| 1/1 [00:00<00:00, 5562.74it/s]\n",
      "Processing test82.txt: 100%|██████████| 1/1 [00:00<00:00, 3968.12it/s]\n",
      "Processing test83.txt: 100%|██████████| 1/1 [00:00<00:00, 5370.43it/s]\n",
      "Processing test84.txt: 100%|██████████| 1/1 [00:00<00:00, 5023.12it/s]\n",
      "Processing test85.txt: 100%|██████████| 1/1 [00:00<00:00, 4092.00it/s]\n",
      "Processing test86.txt: 100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "Processing test87.txt: 100%|██████████| 1/1 [00:00<00:00, 5584.96it/s]\n",
      "Processing test88.txt: 100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "Processing test89.txt: 100%|██████████| 1/1 [00:00<00:00, 3971.88it/s]\n",
      "Processing test90.txt: 100%|██████████| 1/1 [00:00<00:00, 4549.14it/s]\n",
      "Processing test91.txt: 100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "Processing test92.txt: 100%|██████████| 1/1 [00:00<00:00, 6492.73it/s]\n",
      "Processing test93.txt: 100%|██████████| 1/1 [00:00<00:00, 2257.43it/s]\n",
      "Processing test94.txt: 100%|██████████| 1/1 [00:00<00:00, 6944.21it/s]\n",
      "Processing test95.txt: 100%|██████████| 1/1 [00:00<00:00, 4588.95it/s]\n",
      "Processing test96.txt: 100%|██████████| 1/1 [00:00<00:00, 3536.51it/s]\n",
      "Processing test97.txt: 100%|██████████| 1/1 [00:00<00:00, 2368.33it/s]\n",
      "Processing test98.txt: 100%|██████████| 1/1 [00:00<00:00, 2688.66it/s]\n",
      "Processing test99.txt: 100%|██████████| 1/1 [00:00<00:00, 2926.94it/s]\n",
      "Processing test100.txt: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Processing test101.txt: 100%|██████████| 1/1 [00:00<00:00, 2109.81it/s]\n",
      "Processing test102.txt: 100%|██████████| 1/1 [00:00<00:00, 8065.97it/s]\n",
      "Processing test103.txt: 100%|██████████| 1/1 [00:00<00:00, 6204.59it/s]\n",
      "Processing test104.txt: 100%|██████████| 1/1 [00:00<00:00, 4604.07it/s]\n",
      "Processing test105.txt: 100%|██████████| 1/1 [00:00<00:00, 5915.80it/s]\n",
      "Processing test106.txt: 100%|██████████| 1/1 [00:00<00:00, 7584.64it/s]\n",
      "Processing test107.txt: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "Processing test108.txt: 100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n",
      "Processing test109.txt: 100%|██████████| 1/1 [00:00<00:00, 4629.47it/s]\n",
      "Processing test110.txt: 100%|██████████| 1/1 [00:00<00:00, 8322.03it/s]\n",
      "Processing test111.txt: 100%|██████████| 1/1 [00:00<00:00, 3659.95it/s]\n",
      "Processing test112.txt: 100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "Processing test113.txt: 100%|██████████| 1/1 [00:00<00:00, 4969.55it/s]\n",
      "Processing test114.txt: 100%|██████████| 1/1 [00:00<00:00, 3095.43it/s]\n",
      "Processing test115.txt: 100%|██████████| 1/1 [00:00<00:00, 2142.14it/s]\n",
      "Processing test116.txt: 100%|██████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "Processing test117.txt: 100%|██████████| 1/1 [00:00<00:00, 2331.46it/s]\n",
      "Processing test118.txt: 100%|██████████| 1/1 [00:00<00:00, 2739.58it/s]\n",
      "Processing test119.txt: 100%|██████████| 1/1 [00:00<00:00, 2066.16it/s]\n",
      "Processing test120.txt: 100%|██████████| 1/1 [00:00<00:00, 964.43it/s]\n",
      "Processing test121.txt: 100%|██████████| 1/1 [00:00<00:00, 1336.62it/s]\n",
      "Processing test122.txt: 100%|██████████| 1/1 [00:00<00:00, 1290.16it/s]\n",
      "Processing test123.txt: 100%|██████████| 1/1 [00:00<00:00, 1466.54it/s]\n",
      "Processing test124.txt: 100%|██████████| 1/1 [00:00<00:00, 1193.60it/s]\n",
      "Processing test125.txt: 100%|██████████| 1/1 [00:00<00:00, 623.13it/s]\n",
      "Processing test126.txt: 100%|██████████| 1/1 [00:00<00:00, 885.81it/s]\n",
      "Processing test127.txt: 100%|██████████| 1/1 [00:00<00:00, 737.91it/s]\n",
      "Processing test128.txt: 100%|██████████| 1/1 [00:00<00:00, 5077.85it/s]\n",
      "Processing test129.txt: 100%|██████████| 1/1 [00:00<00:00, 3971.88it/s]\n",
      "Processing test130.txt: 100%|██████████| 1/1 [00:00<00:00, 1890.18it/s]\n",
      "Processing test131.txt: 100%|██████████| 1/1 [00:00<00:00, 2768.52it/s]\n",
      "Processing test132.txt: 100%|██████████| 1/1 [00:00<00:00, 5029.14it/s]\n",
      "Processing test133.txt: 100%|██████████| 1/1 [00:00<00:00, 1615.68it/s]\n",
      "Processing test134.txt: 100%|██████████| 1/1 [00:00<00:00, 3184.74it/s]\n",
      "Processing test135.txt: 100%|██████████| 1/1 [00:00<00:00, 4760.84it/s]\n",
      "Processing test136.txt: 100%|██████████| 1/1 [00:00<00:00, 1972.86it/s]\n",
      "Processing test137.txt: 100%|██████████| 1/1 [00:00<00:00, 3653.57it/s]\n",
      "Processing test138.txt: 100%|██████████| 1/1 [00:00<00:00, 4100.00it/s]\n",
      "Processing test139.txt: 100%|██████████| 1/1 [00:00<00:00, 4346.43it/s]\n",
      "Processing test140.txt: 100%|██████████| 1/1 [00:00<00:00, 5377.31it/s]\n",
      "Processing test141.txt: 100%|██████████| 1/1 [00:00<00:00, 1492.10it/s]\n",
      "Processing test142.txt: 100%|██████████| 1/1 [00:00<00:00, 4161.02it/s]\n",
      "Processing test143.txt: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "Processing test144.txt: 100%|██████████| 1/1 [00:00<00:00, 3545.48it/s]\n",
      "Processing test145.txt: 100%|██████████| 1/1 [00:00<00:00, 4760.84it/s]\n",
      "Processing test146.txt: 100%|██████████| 1/1 [00:00<00:00, 3701.95it/s]\n",
      "Processing test147.txt: 100%|██████████| 1/1 [00:00<00:00, 5745.62it/s]\n",
      "Processing test148.txt: 100%|██████████| 1/1 [00:00<00:00, 3320.91it/s]\n",
      "Processing test149.txt: 100%|██████████| 1/1 [00:00<00:00, 3653.57it/s]\n",
      "Processing test150.txt: 100%|██████████| 1/1 [00:00<00:00, 2242.94it/s]\n",
      "Processing test151.txt: 100%|██████████| 1/1 [00:00<00:00, 4132.32it/s]\n",
      "Processing test152.txt: 100%|██████████| 1/1 [00:00<00:00, 2348.43it/s]\n",
      "Processing test153.txt: 100%|██████████| 1/1 [00:00<00:00, 4116.10it/s]\n",
      "Processing test154.txt: 100%|██████████| 1/1 [00:00<00:00, 2597.09it/s]\n",
      "Processing test155.txt: 100%|██████████| 1/1 [00:00<00:00, 1723.92it/s]\n",
      "Processing test156.txt: 100%|██████████| 1/1 [00:00<00:00, 5949.37it/s]\n",
      "Processing test157.txt: 100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "Processing test158.txt: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Processing test159.txt: 100%|██████████| 1/1 [00:00<00:00, 3398.95it/s]\n",
      "Processing test160.txt: 100%|██████████| 1/1 [00:00<00:00, 2618.17it/s]\n",
      "Processing test161.txt: 100%|██████████| 1/1 [00:00<00:00, 2785.06it/s]\n",
      "Processing test162.txt: 100%|██████████| 1/1 [00:00<00:00, 2416.07it/s]\n",
      "Processing test163.txt: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Processing test164.txt: 100%|██████████| 1/1 [00:00<00:00, 2132.34it/s]\n",
      "Processing test165.txt: 100%|██████████| 1/1 [00:00<00:00, 1781.78it/s]\n",
      "Processing test166.txt: 100%|██████████| 1/1 [00:00<00:00, 744.46it/s]\n",
      "Processing test167.txt: 100%|██████████| 1/1 [00:00<00:00, 2880.70it/s]\n",
      "Processing test168.txt: 100%|██████████| 1/1 [00:00<00:00, 3344.74it/s]\n",
      "Processing test169.txt: 100%|██████████| 1/1 [00:00<00:00, 4258.18it/s]\n",
      "Processing test170.txt: 100%|██████████| 1/1 [00:00<00:00, 4922.89it/s]\n",
      "Processing test171.txt: 100%|██████████| 1/1 [00:00<00:00, 2768.52it/s]\n",
      "Processing test172.txt: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "Processing test173.txt: 100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]\n",
      "Processing test174.txt: 100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]\n",
      "Processing test175.txt: 100%|██████████| 1/1 [00:00<00:00, 5614.86it/s]\n",
      "Processing test176.txt: 100%|██████████| 1/1 [00:00<00:00, 3165.51it/s]\n",
      "Processing test177.txt: 100%|██████████| 1/1 [00:00<00:00, 862.67it/s]\n",
      "Processing test178.txt: 100%|██████████| 1/1 [00:00<00:00, 4854.52it/s]\n",
      "Processing test179.txt: 100%|██████████| 1/1 [00:00<00:00, 1655.86it/s]\n",
      "Processing test180.txt: 100%|██████████| 1/1 [00:00<00:00, 1360.46it/s]\n",
      "Processing test181.txt: 100%|██████████| 1/1 [00:00<00:00, 2345.81it/s]\n",
      "Processing test182.txt: 100%|██████████| 1/1 [00:00<00:00, 2688.66it/s]\n",
      "Processing test183.txt: 100%|██████████| 1/1 [00:00<00:00, 3731.59it/s]\n",
      "Processing test184.txt: 100%|██████████| 1/1 [00:00<00:00, 958.70it/s]\n",
      "Processing test185.txt: 100%|██████████| 1/1 [00:00<00:00, 423.71it/s]\n",
      "Processing test186.txt: 100%|██████████| 1/1 [00:00<00:00, 2888.64it/s]\n",
      "Processing test187.txt: 100%|██████████| 1/1 [00:00<00:00, 1203.53it/s]\n",
      "Processing test188.txt: 100%|██████████| 1/1 [00:00<00:00, 1112.25it/s]\n",
      "Processing test189.txt: 100%|██████████| 1/1 [00:00<00:00, 2304.56it/s]\n",
      "Processing test190.txt: 100%|██████████| 1/1 [00:00<00:00, 1318.96it/s]\n",
      "Processing test191.txt: 100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "Processing test192.txt: 100%|██████████| 1/1 [00:00<00:00, 4443.12it/s]\n",
      "Processing test193.txt: 100%|██████████| 1/1 [00:00<00:00, 4100.00it/s]\n",
      "Processing test194.txt: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Processing test195.txt: 100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]\n",
      "Processing test196.txt: 100%|██████████| 1/1 [00:00<00:00, 5377.31it/s]\n",
      "Processing test197.txt: 100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "Processing test198.txt: 100%|██████████| 1/1 [00:00<00:00, 5102.56it/s]\n",
      "Processing test199.txt: 100%|██████████| 1/1 [00:00<00:00, 5737.76it/s]\n",
      "Processing test200.txt: 100%|██████████| 1/1 [00:00<00:00, 2058.05it/s]\n",
      "Processing test201.txt: 100%|██████████| 1/1 [00:00<00:00, 1703.62it/s]\n",
      "Processing test202.txt: 100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]\n",
      "Processing test203.txt: 100%|██████████| 1/1 [00:00<00:00, 3731.59it/s]\n",
      "Processing test204.txt: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "Processing test205.txt: 100%|██████████| 1/1 [00:00<00:00, 5584.96it/s]\n",
      "Processing test206.txt: 100%|██████████| 1/1 [00:00<00:00, 3663.15it/s]\n",
      "Processing test207.txt: 100%|██████████| 1/1 [00:00<00:00, 5384.22it/s]\n",
      "Processing test208.txt: 100%|██████████| 1/1 [00:00<00:00, 3010.99it/s]\n",
      "Processing test209.txt: 100%|██████████| 1/1 [00:00<00:00, 3761.71it/s]\n",
      "Processing test210.txt: 100%|██████████| 1/1 [00:00<00:00, 5440.08it/s]\n",
      "Processing test211.txt: 100%|██████████| 1/1 [00:00<00:00, 4341.93it/s]\n",
      "Processing test212.txt: 100%|██████████| 1/1 [00:00<00:00, 2833.99it/s]\n",
      "Processing test213.txt: 100%|██████████| 1/1 [00:00<00:00, 4364.52it/s]\n",
      "Processing test214.txt: 100%|██████████| 1/1 [00:00<00:00, 4443.12it/s]\n",
      "Processing test215.txt: 100%|██████████| 1/1 [00:00<00:00, 4609.13it/s]\n",
      "Processing test216.txt: 100%|██████████| 1/1 [00:00<00:00, 5809.29it/s]\n",
      "Processing test217.txt: 100%|██████████| 1/1 [00:00<00:00, 3344.74it/s]\n",
      "Processing test218.txt: 100%|██████████| 1/1 [00:00<00:00, 4809.98it/s]\n",
      "Processing test219.txt: 100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]\n",
      "Processing test220.txt: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "Processing test221.txt: 100%|██████████| 1/1 [00:00<00:00, 5645.09it/s]\n",
      "Processing test222.txt: 100%|██████████| 1/1 [00:00<00:00, 4328.49it/s]\n",
      "Processing test223.txt: 100%|██████████| 1/1 [00:00<00:00, 2590.68it/s]\n",
      "Processing test224.txt: 100%|██████████| 1/1 [00:00<00:00, 4609.13it/s]\n",
      "Processing test225.txt: 100%|██████████| 1/1 [00:00<00:00, 3079.52it/s]\n",
      "Processing test226.txt: 100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "Processing test227.txt: 100%|██████████| 1/1 [00:00<00:00, 6026.30it/s]\n",
      "Processing test228.txt: 100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "Processing test229.txt: 100%|██████████| 1/1 [00:00<00:00, 6186.29it/s]\n",
      "Processing test230.txt: 100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "Processing test231.txt: 100%|██████████| 1/1 [00:00<00:00, 2632.96it/s]\n",
      "Processing test232.txt: 100%|██████████| 1/1 [00:00<00:00, 3847.99it/s]\n",
      "Processing test233.txt: 100%|██████████| 1/1 [00:00<00:00, 5289.16it/s]\n",
      "Processing test234.txt: 100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "Processing test235.txt: 100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "Processing test236.txt: 100%|██████████| 1/1 [00:00<00:00, 4696.87it/s]\n",
      "Processing test237.txt: 100%|██████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "Processing test238.txt: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Processing test239.txt: 100%|██████████| 1/1 [00:00<00:00, 2173.21it/s]\n",
      "Processing test240.txt: 100%|██████████| 1/1 [00:00<00:00, 4080.06it/s]\n",
      "Processing test241.txt: 100%|██████████| 1/1 [00:00<00:00, 5745.62it/s]\n",
      "Processing test242.txt: 100%|██████████| 1/1 [00:00<00:00, 3758.34it/s]\n",
      "Processing test243.txt: 100%|██████████| 1/1 [00:00<00:00, 3390.71it/s]\n",
      "Processing test244.txt: 100%|██████████| 1/1 [00:00<00:00, 596.04it/s]\n",
      "Processing test245.txt: 100%|██████████| 1/1 [00:00<00:00, 1321.04it/s]\n",
      "Processing test246.txt: 100%|██████████| 1/1 [00:00<00:00, 2288.22it/s]\n",
      "Processing test247.txt: 100%|██████████| 1/1 [00:00<00:00, 2262.30it/s]\n",
      "Processing test248.txt: 100%|██████████| 1/1 [00:00<00:00, 1555.75it/s]\n",
      "Processing test249.txt: 100%|██████████| 1/1 [00:00<00:00, 3412.78it/s]\n",
      "Processing test250.txt: 100%|██████████| 1/1 [00:00<00:00, 2746.76it/s]\n",
      "Processing test251.txt: 100%|██████████| 1/1 [00:00<00:00, 3077.26it/s]\n",
      "Processing test252.txt: 100%|██████████| 1/1 [00:00<00:00, 2681.78it/s]\n",
      "Processing test253.txt: 100%|██████████| 1/1 [00:00<00:00, 2451.38it/s]\n",
      "Processing test254.txt: 100%|██████████| 1/1 [00:00<00:00, 2178.86it/s]\n",
      "Processing test255.txt: 100%|██████████| 1/1 [00:00<00:00, 3938.31it/s]\n",
      "Processing test256.txt: 100%|██████████| 1/1 [00:00<00:00, 3182.32it/s]\n",
      "Processing test257.txt: 100%|██████████| 1/1 [00:00<00:00, 2388.56it/s]\n",
      "Processing test258.txt: 100%|██████████| 1/1 [00:00<00:00, 933.52it/s]\n",
      "Processing test259.txt: 100%|██████████| 1/1 [00:00<00:00, 2763.05it/s]\n",
      "Processing test260.txt: 100%|██████████| 1/1 [00:00<00:00, 2428.66it/s]\n",
      "Processing test261.txt: 100%|██████████| 1/1 [00:00<00:00, 2283.24it/s]\n",
      "Processing test262.txt: 100%|██████████| 1/1 [00:00<00:00, 2654.62it/s]\n",
      "Processing test263.txt: 100%|██████████| 1/1 [00:00<00:00, 4369.07it/s]\n",
      "Processing test264.txt: 100%|██████████| 1/1 [00:00<00:00, 4443.12it/s]\n",
      "Processing test265.txt: 100%|██████████| 1/1 [00:00<00:00, 1567.38it/s]\n",
      "Processing test266.txt: 100%|██████████| 1/1 [00:00<00:00, 1941.81it/s]\n",
      "Processing test267.txt: 100%|██████████| 1/1 [00:00<00:00, 807.06it/s]\n",
      "Processing test268.txt: 100%|██████████| 1/1 [00:00<00:00, 753.69it/s]\n",
      "Processing test269.txt: 100%|██████████| 1/1 [00:00<00:00, 4328.49it/s]\n",
      "Processing test270.txt: 100%|██████████| 1/1 [00:00<00:00, 4670.72it/s]\n",
      "Processing test271.txt: 100%|██████████| 1/1 [00:00<00:00, 4860.14it/s]\n",
      "Processing test272.txt: 100%|██████████| 1/1 [00:00<00:00, 4485.89it/s]\n",
      "Processing test273.txt: 100%|██████████| 1/1 [00:00<00:00, 5322.72it/s]\n",
      "Processing test274.txt: 100%|██████████| 1/1 [00:00<00:00, 7410.43it/s]\n",
      "Processing test275.txt: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "Processing test276.txt: 100%|██████████| 1/1 [00:00<00:00, 5489.93it/s]\n",
      "Processing test277.txt: 100%|██████████| 1/1 [00:00<00:00, 4681.14it/s]\n",
      "Processing test278.txt: 100%|██████████| 1/1 [00:00<00:00, 4837.72it/s]\n",
      "Processing test279.txt: 100%|██████████| 1/1 [00:00<00:00, 5029.14it/s]\n",
      "Processing test280.txt: 100%|██████████| 1/1 [00:00<00:00, 1090.56it/s]\n",
      "Processing test281.txt: 100%|██████████| 1/1 [00:00<00:00, 1141.62it/s]\n",
      "Processing test282.txt: 100%|██████████| 1/1 [00:00<00:00, 3572.66it/s]\n",
      "Processing test283.txt: 100%|██████████| 1/1 [00:00<00:00, 1040.77it/s]\n",
      "Processing test284.txt: 100%|██████████| 1/1 [00:00<00:00, 2150.93it/s]\n",
      "Processing test285.txt: 100%|██████████| 1/1 [00:00<00:00, 2590.68it/s]\n",
      "Processing test286.txt: 100%|██████████| 1/1 [00:00<00:00, 920.81it/s]\n",
      "Processing test287.txt: 100%|██████████| 1/1 [00:00<00:00, 1529.09it/s]\n",
      "Processing test288.txt: 100%|██████████| 1/1 [00:00<00:00, 1949.03it/s]\n",
      "Processing test289.txt: 100%|██████████| 1/1 [00:00<00:00, 438.18it/s]\n",
      "Processing test290.txt: 100%|██████████| 1/1 [00:00<00:00, 696.38it/s]\n",
      "Processing test291.txt: 100%|██████████| 1/1 [00:00<00:00, 477.17it/s]\n",
      "Processing test292.txt: 100%|██████████| 1/1 [00:00<00:00, 3116.12it/s]\n",
      "Processing test293.txt: 100%|██████████| 1/1 [00:00<00:00, 7244.05it/s]\n",
      "Processing test294.txt: 100%|██████████| 1/1 [00:00<00:00, 3379.78it/s]\n",
      "Processing test295.txt: 100%|██████████| 1/1 [00:00<00:00, 2216.86it/s]\n",
      "Processing test296.txt: 100%|██████████| 1/1 [00:00<00:00, 4315.13it/s]\n",
      "Processing test297.txt: 100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]\n",
      "Processing test298.txt: 100%|██████████| 1/1 [00:00<00:00, 5497.12it/s]\n",
      "Processing test299.txt: 100%|██████████| 1/1 [00:00<00:00, 188.81it/s]\n",
      "Processing test300.txt: 100%|██████████| 1/1 [00:00<00:00, 1875.81it/s]\n",
      "Processing test301.txt: 100%|██████████| 1/1 [00:00<00:00, 3258.98it/s]\n",
      "Processing test302.txt: 100%|██████████| 1/1 [00:00<00:00, 5412.01it/s]\n",
      "Processing test303.txt: 100%|██████████| 1/1 [00:00<00:00, 7410.43it/s]\n",
      "Processing test304.txt: 100%|██████████| 1/1 [00:00<00:00, 3816.47it/s]\n",
      "Processing test305.txt: 100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "Processing test306.txt: 100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "Processing test307.txt: 100%|██████████| 1/1 [00:00<00:00, 6543.38it/s]\n",
      "Processing test308.txt: 100%|██████████| 1/1 [00:00<00:00, 5295.84it/s]\n",
      "Processing test309.txt: 100%|██████████| 1/1 [00:00<00:00, 3002.37it/s]\n",
      "Processing test310.txt: 100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "Processing test311.txt: 100%|██████████| 1/1 [00:00<00:00, 3844.46it/s]\n",
      "Processing test312.txt: 100%|██████████| 1/1 [00:00<00:00, 3300.00it/s]\n",
      "Processing test313.txt: 100%|██████████| 1/1 [00:00<00:00, 4809.98it/s]\n",
      "Processing test314.txt: 100%|██████████| 1/1 [00:00<00:00, 4096.00it/s]\n",
      "Processing test315.txt: 100%|██████████| 1/1 [00:00<00:00, 1824.40it/s]\n",
      "Processing test316.txt: 100%|██████████| 1/1 [00:00<00:00, 5412.01it/s]\n",
      "Processing test317.txt: 100%|██████████| 1/1 [00:00<00:00, 2304.56it/s]\n",
      "Processing test318.txt: 100%|██████████| 1/1 [00:00<00:00, 2037.06it/s]\n",
      "Processing test319.txt: 100%|██████████| 1/1 [00:00<00:00, 825.16it/s]\n",
      "Processing test320.txt: 100%|██████████| 1/1 [00:00<00:00, 977.47it/s]\n",
      "Processing test321.txt: 100%|██████████| 1/1 [00:00<00:00, 1105.22it/s]\n",
      "Processing test322.txt: 100%|██████████| 1/1 [00:00<00:00, 3030.57it/s]\n",
      "Processing test323.txt: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Processing test324.txt: 100%|██████████| 1/1 [00:00<00:00, 4116.10it/s]\n",
      "Processing test325.txt: 100%|██████████| 1/1 [00:00<00:00, 6944.21it/s]\n",
      "Processing test326.txt: 100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "Processing test327.txt: 100%|██████████| 1/1 [00:00<00:00, 3236.35it/s]\n",
      "Processing test328.txt: 100%|██████████| 1/1 [00:00<00:00, 3557.51it/s]\n",
      "Processing test329.txt: 100%|██████████| 1/1 [00:00<00:00, 5915.80it/s]\n",
      "Processing test330.txt: 100%|██████████| 1/1 [00:00<00:00, 4946.11it/s]\n",
      "Processing test331.txt: 100%|██████████| 1/1 [00:00<00:00, 3905.31it/s]\n",
      "Processing test332.txt: 100%|██████████| 1/1 [00:00<00:00, 4116.10it/s]\n",
      "Processing test333.txt: 100%|██████████| 1/1 [00:00<00:00, 4048.56it/s]\n",
      "Processing test334.txt: 100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]\n",
      "Processing test335.txt: 100%|██████████| 1/1 [00:00<00:00, 3847.99it/s]\n",
      "Processing test336.txt: 100%|██████████| 1/1 [00:00<00:00, 2957.90it/s]\n",
      "Processing test337.txt: 100%|██████████| 1/1 [00:00<00:00, 1834.78it/s]\n",
      "Processing test338.txt: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Processing test339.txt: 100%|██████████| 1/1 [00:00<00:00, 2080.51it/s]\n",
      "Processing test340.txt: 100%|██████████| 1/1 [00:00<00:00, 2666.44it/s]\n",
      "Processing test341.txt: 100%|██████████| 1/1 [00:00<00:00, 1457.87it/s]\n",
      "Processing test342.txt: 100%|██████████| 1/1 [00:00<00:00, 2082.57it/s]\n",
      "Processing test343.txt: 100%|██████████| 1/1 [00:00<00:00, 2714.76it/s]\n",
      "Processing test344.txt: 100%|██████████| 1/1 [00:00<00:00, 3269.14it/s]\n",
      "Processing test345.txt: 100%|██████████| 1/1 [00:00<00:00, 1406.07it/s]\n",
      "Processing test346.txt: 100%|██████████| 1/1 [00:00<00:00, 1414.61it/s]\n",
      "Processing test347.txt: 100%|██████████| 1/1 [00:00<00:00, 3968.12it/s]\n",
      "Processing test348.txt: 100%|██████████| 1/1 [00:00<00:00, 5121.25it/s]\n",
      "Processing test349.txt: 100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "Processing test350.txt: 100%|██████████| 1/1 [00:00<00:00, 3830.41it/s]\n",
      "Processing test351.txt: 100%|██████████| 1/1 [00:00<00:00, 3118.44it/s]\n",
      "Processing test352.txt: 100%|██████████| 1/1 [00:00<00:00, 3423.92it/s]\n",
      "Processing test353.txt: 100%|██████████| 1/1 [00:00<00:00, 5309.25it/s]\n",
      "Processing test354.txt: 100%|██████████| 1/1 [00:00<00:00, 730.46it/s]\n",
      "Processing test355.txt: 100%|██████████| 1/1 [00:00<00:00, 414.78it/s]\n",
      "Processing test356.txt: 100%|██████████| 1/1 [00:00<00:00, 3771.86it/s]\n",
      "Processing test357.txt: 100%|██████████| 1/1 [00:00<00:00, 4696.87it/s]\n",
      "Processing test358.txt: 100%|██████████| 1/1 [00:00<00:00, 1607.63it/s]\n",
      "Processing test359.txt: 100%|██████████| 1/1 [00:00<00:00, 1285.41it/s]\n",
      "Processing test360.txt: 100%|██████████| 1/1 [00:00<00:00, 1453.33it/s]\n",
      "Processing test361.txt: 100%|██████████| 1/1 [00:00<00:00, 4401.16it/s]\n",
      "Processing test362.txt: 100%|██████████| 1/1 [00:00<00:00, 3908.95it/s]\n",
      "Processing test363.txt: 100%|██████████| 1/1 [00:00<00:00, 4002.20it/s]\n",
      "Processing test364.txt: 100%|██████████| 1/1 [00:00<00:00, 3557.51it/s]\n",
      "Processing test365.txt: 100%|██████████| 1/1 [00:00<00:00, 4419.71it/s]\n",
      "Processing test366.txt: 100%|██████████| 1/1 [00:00<00:00, 3938.31it/s]\n",
      "Processing test367.txt: 100%|██████████| 1/1 [00:00<00:00, 1011.16it/s]\n",
      "Processing test368.txt: 100%|██████████| 1/1 [00:00<00:00, 5691.05it/s]\n",
      "Processing test369.txt: 100%|██████████| 1/1 [00:00<00:00, 3331.46it/s]\n",
      "Processing test370.txt: 100%|██████████| 1/1 [00:00<00:00, 3953.16it/s]\n",
      "Processing test371.txt: 100%|██████████| 1/1 [00:00<00:00, 1027.76it/s]\n",
      "Processing test372.txt: 100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "Processing test373.txt: 100%|██████████| 1/1 [00:00<00:00, 4675.92it/s]\n",
      "Processing test374.txt: 100%|██████████| 1/1 [00:00<00:00, 3826.92it/s]\n",
      "Processing test375.txt: 100%|██████████| 1/1 [00:00<00:00, 1957.21it/s]\n",
      "Processing test376.txt: 100%|██████████| 1/1 [00:00<00:00, 2657.99it/s]\n",
      "Processing test377.txt: 100%|██████████| 1/1 [00:00<00:00, 1096.26it/s]\n",
      "Processing test378.txt: 100%|██████████| 1/1 [00:00<00:00, 573.15it/s]\n",
      "Processing test379.txt: 100%|██████████| 1/1 [00:00<00:00, 2445.66it/s]\n",
      "Processing test380.txt: 100%|██████████| 1/1 [00:00<00:00, 910.02it/s]\n",
      "Processing test381.txt: 100%|██████████| 1/1 [00:00<00:00, 1976.58it/s]\n",
      "Processing test382.txt: 100%|██████████| 1/1 [00:00<00:00, 1021.51it/s]\n",
      "Processing test383.txt: 100%|██████████| 1/1 [00:00<00:00, 1655.21it/s]\n",
      "Processing test384.txt: 100%|██████████| 1/1 [00:00<00:00, 158.68it/s]\n",
      "Processing test385.txt: 100%|██████████| 1/1 [00:00<00:00, 2892.62it/s]\n",
      "Processing test386.txt: 100%|██████████| 1/1 [00:00<00:00, 682.11it/s]\n",
      "Processing test387.txt: 100%|██████████| 1/1 [00:00<00:00, 1971.93it/s]\n",
      "Processing test388.txt: 100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]\n",
      "Processing test389.txt: 100%|██████████| 1/1 [00:00<00:00, 1551.15it/s]\n",
      "Processing test390.txt: 100%|██████████| 1/1 [00:00<00:00, 5745.62it/s]\n",
      "Processing test391.txt: 100%|██████████| 1/1 [00:00<00:00, 5077.85it/s]\n",
      "Processing test392.txt: 100%|██████████| 1/1 [00:00<00:00, 2299.51it/s]\n",
      "Processing test393.txt: 100%|██████████| 1/1 [00:00<00:00, 6842.26it/s]\n",
      "Processing test394.txt: 100%|██████████| 1/1 [00:00<00:00, 3788.89it/s]\n",
      "Processing test395.txt: 100%|██████████| 1/1 [00:00<00:00, 2659.67it/s]\n",
      "Processing test396.txt: 100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n",
      "Processing test397.txt: 100%|██████████| 1/1 [00:00<00:00, 1615.68it/s]\n",
      "Processing test398.txt: 100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "Processing test399.txt: 100%|██████████| 1/1 [00:00<00:00, 4198.50it/s]\n",
      "Processing test400.txt: 100%|██████████| 1/1 [00:00<00:00, 1739.65it/s]\n",
      "Processing test401.txt: 100%|██████████| 1/1 [00:00<00:00, 1254.65it/s]\n",
      "Processing test402.txt: 100%|██████████| 1/1 [00:00<00:00, 3045.97it/s]\n",
      "Processing test403.txt: 100%|██████████| 1/1 [00:00<00:00, 3905.31it/s]\n",
      "Processing test404.txt: 100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]\n",
      "Processing test405.txt: 100%|██████████| 1/1 [00:00<00:00, 2531.26it/s]\n",
      "Processing test406.txt: 100%|██████████| 1/1 [00:00<00:00, 1051.73it/s]\n",
      "Processing test407.txt: 100%|██████████| 1/1 [00:00<00:00, 3194.44it/s]\n",
      "Processing test408.txt: 100%|██████████| 1/1 [00:00<00:00, 1607.63it/s]\n",
      "Processing test409.txt: 100%|██████████| 1/1 [00:00<00:00, 1277.58it/s]\n",
      "Processing test410.txt: 100%|██████████| 1/1 [00:00<00:00, 453.54it/s]\n",
      "Processing test411.txt: 100%|██████████| 1/1 [00:00<00:00, 1213.98it/s]\n",
      "Processing test412.txt: 100%|██████████| 1/1 [00:00<00:00, 4975.45it/s]\n",
      "Processing test413.txt: 100%|██████████| 1/1 [00:00<00:00, 3030.57it/s]\n",
      "Processing test414.txt: 100%|██████████| 1/1 [00:00<00:00, 2222.74it/s]\n",
      "Processing test415.txt: 100%|██████████| 1/1 [00:00<00:00, 3002.37it/s]\n",
      "Processing test416.txt: 100%|██████████| 1/1 [00:00<00:00, 2801.81it/s]\n",
      "Processing test417.txt: 100%|██████████| 1/1 [00:00<00:00, 950.44it/s]\n",
      "Processing test418.txt: 100%|██████████| 1/1 [00:00<00:00, 3521.67it/s]\n",
      "Processing test419.txt: 100%|██████████| 1/1 [00:00<00:00, 4410.41it/s]\n",
      "Processing test420.txt: 100%|██████████| 1/1 [00:00<00:00, 5349.88it/s]\n",
      "Processing test421.txt: 100%|██████████| 1/1 [00:00<00:00, 3355.44it/s]\n",
      "Processing test422.txt: 100%|██████████| 1/1 [00:00<00:00, 2551.28it/s]\n",
      "Processing test423.txt: 100%|██████████| 1/1 [00:00<00:00, 3002.37it/s]\n",
      "Processing test424.txt: 100%|██████████| 1/1 [00:00<00:00, 4275.54it/s]\n",
      "Processing test425.txt: 100%|██████████| 1/1 [00:00<00:00, 1521.88it/s]\n",
      "Processing test426.txt: 100%|██████████| 1/1 [00:00<00:00, 2262.30it/s]\n",
      "Processing test427.txt: 100%|██████████| 1/1 [00:00<00:00, 2427.26it/s]\n",
      "Processing test428.txt: 100%|██████████| 1/1 [00:00<00:00, 4975.45it/s]\n",
      "Processing test429.txt: 100%|██████████| 1/1 [00:00<00:00, 3905.31it/s]\n",
      "Processing test430.txt: 100%|██████████| 1/1 [00:00<00:00, 4236.67it/s]\n",
      "Processing test431.txt: 100%|██████████| 1/1 [00:00<00:00, 4405.78it/s]\n",
      "Processing test432.txt: 100%|██████████| 1/1 [00:00<00:00, 2949.58it/s]\n",
      "Processing test433.txt: 100%|██████████| 1/1 [00:00<00:00, 1883.39it/s]\n",
      "Processing test434.txt: 100%|██████████| 1/1 [00:00<00:00, 4854.52it/s]\n",
      "Processing test435.txt: 100%|██████████| 1/1 [00:00<00:00, 2571.61it/s]\n",
      "Processing test436.txt: 100%|██████████| 1/1 [00:00<00:00, 5289.16it/s]\n",
      "Processing test437.txt: 100%|██████████| 1/1 [00:00<00:00, 4539.29it/s]\n",
      "Processing test438.txt: 100%|██████████| 1/1 [00:00<00:00, 5461.33it/s]\n",
      "Processing test439.txt: 100%|██████████| 1/1 [00:00<00:00, 1975.65it/s]\n",
      "Processing test440.txt: 100%|██████████| 1/1 [00:00<00:00, 2118.34it/s]\n",
      "Processing test441.txt: 100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]\n",
      "Processing test442.txt: 100%|██████████| 1/1 [00:00<00:00, 4387.35it/s]\n",
      "Processing test443.txt: 100%|██████████| 1/1 [00:00<00:00, 2693.84it/s]\n",
      "Processing test444.txt: 100%|██████████| 1/1 [00:00<00:00, 3983.19it/s]\n",
      "Processing test445.txt: 100%|██████████| 1/1 [00:00<00:00, 2434.30it/s]\n",
      "Processing test446.txt: 100%|██████████| 1/1 [00:00<00:00, 3437.95it/s]\n",
      "Processing test447.txt: 100%|██████████| 1/1 [00:00<00:00, 4017.53it/s]\n",
      "Processing test448.txt: 100%|██████████| 1/1 [00:00<00:00, 3518.71it/s]\n",
      "Processing test449.txt: 100%|██████████| 1/1 [00:00<00:00, 7449.92it/s]\n",
      "Processing test450.txt: 100%|██████████| 1/1 [00:00<00:00, 2173.21it/s]\n",
      "Processing test451.txt: 100%|██████████| 1/1 [00:00<00:00, 2232.20it/s]\n",
      "Processing test452.txt: 100%|██████████| 1/1 [00:00<00:00, 4112.06it/s]\n",
      "Processing test453.txt: 100%|██████████| 1/1 [00:00<00:00, 3334.10it/s]\n",
      "Processing test454.txt: 100%|██████████| 1/1 [00:00<00:00, 3452.10it/s]\n",
      "Processing test455.txt: 100%|██████████| 1/1 [00:00<00:00, 3066.01it/s]\n",
      "Processing test456.txt: 100%|██████████| 1/1 [00:00<00:00, 2206.37it/s]\n",
      "Processing test457.txt: 100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]\n",
      "Processing test458.txt: 100%|██████████| 1/1 [00:00<00:00, 798.00it/s]\n",
      "Processing test459.txt: 100%|██████████| 1/1 [00:00<00:00, 1464.49it/s]\n",
      "Processing test460.txt: 100%|██████████| 1/1 [00:00<00:00, 3545.48it/s]\n",
      "Processing test461.txt: 100%|██████████| 1/1 [00:00<00:00, 3833.92it/s]\n",
      "Processing test462.txt: 100%|██████████| 1/1 [00:00<00:00, 2283.24it/s]\n",
      "Processing test463.txt: 100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]\n",
      "Processing test464.txt: 100%|██████████| 1/1 [00:00<00:00, 6096.37it/s]\n",
      "Processing test465.txt: 100%|██████████| 1/1 [00:00<00:00, 4293.04it/s]\n",
      "Processing test466.txt: 100%|██████████| 1/1 [00:00<00:00, 4609.13it/s]\n",
      "Processing test467.txt: 100%|██████████| 1/1 [00:00<00:00, 4815.50it/s]\n",
      "Processing test468.txt: 100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n",
      "Processing test469.txt: 100%|██████████| 1/1 [00:00<00:00, 3297.41it/s]\n",
      "Processing test470.txt: 100%|██████████| 1/1 [00:00<00:00, 2922.86it/s]\n",
      "Processing test471.txt: 100%|██████████| 1/1 [00:00<00:00, 4275.54it/s]\n",
      "Processing test472.txt: 100%|██████████| 1/1 [00:00<00:00, 3847.99it/s]\n",
      "Processing test473.txt: 100%|██████████| 1/1 [00:00<00:00, 5029.14it/s]\n",
      "Processing test474.txt: 100%|██████████| 1/1 [00:00<00:00, 1019.27it/s]\n",
      "Processing test475.txt: 100%|██████████| 1/1 [00:00<00:00, 1615.06it/s]\n",
      "Processing test476.txt: 100%|██████████| 1/1 [00:00<00:00, 1731.04it/s]\n",
      "Processing test477.txt: 100%|██████████| 1/1 [00:00<00:00, 4096.00it/s]\n",
      "Processing test478.txt: 100%|██████████| 1/1 [00:00<00:00, 2261.08it/s]\n",
      "Processing test479.txt: 100%|██████████| 1/1 [00:00<00:00, 6533.18it/s]\n",
      "Processing test480.txt: 100%|██████████| 1/1 [00:00<00:00, 4116.10it/s]\n",
      "Processing test481.txt: 100%|██████████| 1/1 [00:00<00:00, 3246.37it/s]\n",
      "Processing test482.txt: 100%|██████████| 1/1 [00:00<00:00, 4387.35it/s]\n",
      "Processing test483.txt: 100%|██████████| 1/1 [00:00<00:00, 713.80it/s]\n",
      "Processing test484.txt: 100%|██████████| 1/1 [00:00<00:00, 1730.32it/s]\n",
      "Processing test485.txt: 100%|██████████| 1/1 [00:00<00:00, 1508.20it/s]\n",
      "Processing test486.txt: 100%|██████████| 1/1 [00:00<00:00, 1822.03it/s]\n",
      "Processing test487.txt: 100%|██████████| 1/1 [00:00<00:00, 1362.67it/s]\n",
      "Processing test488.txt: 100%|██████████| 1/1 [00:00<00:00, 3460.65it/s]\n",
      "Processing test489.txt: 100%|██████████| 1/1 [00:00<00:00, 162.18it/s]\n",
      "Processing test490.txt: 100%|██████████| 1/1 [00:00<00:00, 2213.35it/s]\n",
      "Processing test491.txt: 100%|██████████| 1/1 [00:00<00:00, 1897.02it/s]\n",
      "Processing test492.txt: 100%|██████████| 1/1 [00:00<00:00, 3175.10it/s]\n",
      "Processing test493.txt: 100%|██████████| 1/1 [00:00<00:00, 4219.62it/s]\n",
      "Processing test494.txt: 100%|██████████| 1/1 [00:00<00:00, 4443.12it/s]\n",
      "Processing test495.txt: 100%|██████████| 1/1 [00:00<00:00, 2507.06it/s]\n",
      "Processing test496.txt: 100%|██████████| 1/1 [00:00<00:00, 3289.65it/s]\n",
      "Processing test497.txt: 100%|██████████| 1/1 [00:00<00:00, 1314.42it/s]\n",
      "Processing test498.txt: 100%|██████████| 1/1 [00:00<00:00, 1831.57it/s]\n",
      "Processing test499.txt: 100%|██████████| 1/1 [00:00<00:00, 4826.59it/s]\n",
      "Processing test500.txt: 100%|██████████| 1/1 [00:00<00:00, 1481.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Gryphe/MythoMax-L2-13b\"\n",
    ")  # It doesn't matter what model goes here, really\n",
    "\n",
    "sentence_chunks = []\n",
    "for source_text in source_texts:\n",
    "    sentence_chunks += control_flow_functions.sentence_chunking_algorithm(source_text, tokenizer)\n",
    "\n",
    "conversions = [(\"\\n\", \" \"), (\"  \", \" \")]\n",
    "\n",
    "paragraphs_processed = [\n",
    "    (control_flow_functions.fix_text(conversions, seq[0]), seq[1]) for seq in sentence_chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect various features of the text you have fed in to see if it came out alright-ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('foo', 'text1')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below cell will take a while to start generating for various screwy async reasons. It's doing its job, it just schedules everything first and THEN you see results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "output_dir = \"./worthy_for_questions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Determine which paragraphs are worthy of making questions from\n",
    "judged_worthy_for_questions = []\n",
    "\n",
    "await control_flow_functions.filter_all_questions(paragraphs_processed, judged_worthy_for_questions, engine_wrapper, output_dir, take_subset=USE_SUBSET, use_filenames=True, rtwl=run_task_with_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_worthy_for_questions = control_flow_functions.filter_and_graph(judged_worthy_for_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_worthy_for_questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below begins generating questions. SOME OF THESE MAY FAIL and have to retry due to model errors (the API branch cannot use grammars). But if you let it run you will see that the vast majority eventually get through.\n",
    "\n",
    "In short, don't get scared by tracebacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control flow\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Directory for QA tuples\n",
    "qa_tuples_dir = \"./qatuples_raw\"\n",
    "if not os.path.exists(qa_tuples_dir):\n",
    "    os.makedirs(qa_tuples_dir)\n",
    "\n",
    "# Initialize vetted_qa_tuples\n",
    "vetted_qa_tuples = []  # tuple list of qa tuples that have been judged good\n",
    "\n",
    "# Attempt to initialize filtered_worthy_for_questions\n",
    "try:\n",
    "    _ = filtered_worthy_for_questions\n",
    "except NameError:\n",
    "    filtered_worthy_for_questions = []\n",
    "\n",
    "if not filtered_worthy_for_questions:\n",
    "    # Load all files in the qa_tuples_dir if filtered_worthy_for_questions is not initialized\n",
    "    existing_files = glob.glob(os.path.join(qa_tuples_dir, \"*.json\"))\n",
    "    for file_path in existing_files:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            qa_tuple = tuple(json.load(file))\n",
    "            print(f\"Loaded {file}\")\n",
    "        vetted_qa_tuples.append(qa_tuple)\n",
    "else:\n",
    "    tasks = [control_flow_functions.generate_qatuples_from_para(\n",
    "        idx,\n",
    "        para,\n",
    "        engine_wrapper=engine_wrapper,\n",
    "        vetted_qa_tuples=vetted_qa_tuples,\n",
    "        qa_tuples_dir=qa_tuples_dir,\n",
    "        double_check_counter=DOUBLE_CHECK_COUNTER,\n",
    "        use_filenames=USE_FILENAMES) for idx,para in enumerate(filtered_worthy_for_questions)]\n",
    "    limited_tasks_qgen = [run_task_with_limit(task) for task in tasks]\n",
    "    for future in tqdmasyncio.tqdm.as_completed(limited_tasks_qgen):\n",
    "            await future\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"-------------- QUESTIONS CREATED ------------- STATS SO FAR (may be wrong if run was continued from interruption):\"\n",
    ")\n",
    "nones = list(filter(lambda x: x[0] is None, vetted_qa_tuples))\n",
    "print(f\"Nones: {len(nones)}\")\n",
    "print(f\"Non-nones: {len(vetted_qa_tuples) - len(nones)}\")\n",
    "print(f\"Total: {len(vetted_qa_tuples)}\")\n",
    "# filter out all None values\n",
    "vetted_qa_tuples = [qa for qa in vetted_qa_tuples if qa[0] is not None]\n",
    "print(\"---------------- ONTO EXAMPLES GENERATION-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and fix the common mistake: mentioning \"the text\".\n",
    "writepath = \"./qatuples_revised\"\n",
    "import json\n",
    "\n",
    "# Assuming vetted_qa_tuples is a list that might or might not exist\n",
    "try:\n",
    "    _ = vetted_qa_tuples\n",
    "except NameError:\n",
    "    vetted_qa_tuples = []\n",
    "\n",
    "# Load all files at the start if vetted_qa_tuples is empty\n",
    "if not vetted_qa_tuples:\n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(writepath):\n",
    "        # List all files in directory\n",
    "        for file_name in os.listdir(writepath):\n",
    "            file_path = os.path.join(writepath, file_name)\n",
    "            try: # for each file already generated, see if it succeeded or failed; if it succeeded, append its contents; if it failed, append None for stats logging\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                    print(f\"Loading file: {file_path}\")\n",
    "                    if content == \"failed\":\n",
    "                        vetted_qa_tuples.append(None)\n",
    "                    else:\n",
    "                        try:\n",
    "                            data = json.loads(content)\n",
    "                            vetted_qa_tuples.append(\n",
    "                                (data[0], data[1], data[2], data[3])\n",
    "                            )\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(\"JSON decode error with the contents:\", content)\n",
    "                            vetted_qa_tuples.append(None)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "else:\n",
    "    old_tuples = vetted_qa_tuples.copy()\n",
    "    tasks = [control_flow_functions.repair_qatuple_context(idx, tup, engine_wrapper, writepath, vetted_qa_tuples,use_filenames=USE_FILENAMES) for idx, tup in enumerate(vetted_qa_tuples)]\n",
    "    limited_tasks_qcorrection = [run_task_with_limit(task) for task in tasks]\n",
    "    for future in tqdmasyncio.tqdm.as_completed(limited_tasks_qcorrection): \n",
    "        await future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats related to revised qatuples, and filter out nones (questions that were unanswerable due to lack of context).\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"-------------- QUESTIONS REVISED ------------- STATS SO FAR:\")\n",
    "nones = list(filter(lambda x: x is None, vetted_qa_tuples))\n",
    "print(f\"Nones: {len(nones)}\")\n",
    "print(f\"Non-nones: {len(vetted_qa_tuples) - len(nones)}\")\n",
    "print(f\"Total: {len(vetted_qa_tuples)}\")\n",
    "# filter out all None values\n",
    "vetted_qa_tuples = [qa for qa in vetted_qa_tuples if qa is not None]\n",
    "print(\"---------------- ONTO EXAMPLES GENERATION-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_tuples_by_paragraph = control_flow_functions.group_by_text(vetted_qa_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(multi_turn_convs_info_dir):\n",
    "    os.makedirs(multi_turn_convs_info_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "multi_turn_convs_info = []\n",
    "\n",
    "\n",
    "tasks = [control_flow_functions.create_info(idx,group,engine_wrapper, ASSISTANT_MODE, multi_turn_convs_info,multi_turn_convs_info_dir, REARRANGEMENTS_TO_TAKE,USE_FILENAMES) for idx,group in enumerate(qa_tuples_by_paragraph)]\n",
    "limited_tasks_infocreation = [run_task_with_limit(task) for task in tasks]\n",
    "for future in tqdmasyncio.tqdm.as_completed(limited_tasks_infocreation):\n",
    "    await future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No notebook restart needed for API notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUT You still might've separated your generations into large and small models, so we reinitialize the engine wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API Client\n",
    "engine_wrapper = EngineWrapper(model=LARGE_LOGICAL_MODEL,api_key=API_KEY,base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "convs_info = control_flow_functions.read_json_files_info(multi_turn_convs_info_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import asyncio\n",
    "\n",
    "multi_turn_convs_dir = \"./multi_turn_convs\"\n",
    "if not os.path.exists(multi_turn_convs_dir):\n",
    "    os.makedirs(multi_turn_convs_dir)\n",
    "\n",
    "multi_turn_convs = []\n",
    "\n",
    "tasks = [control_flow_functions.create_conversation(idx,info, engine_wrapper, multi_turn_convs, multi_turn_convs_dir, ASSISTANT_MODE) for idx,info in enumerate(convs_info)]\n",
    "limited_tasks_convwriting = [run_task_with_limit(task) for task in tasks]\n",
    "for future in tqdmasyncio.tqdm.as_completed(limited_tasks_convwriting):\n",
    "    await future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yay! Now you have a dataset!\n",
    "### GPT wrote the cell below. I think it successfully converts things to ShareGPT format for use with axolotl, but I am not sure because I don't know that format very well and haven't used Axolotl. However, the json produced by the second function looks fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Make ShareGPT-format dataset (I think, still need verification it actually works)\n",
    "control_flow_functions.convert_directory_to_list(\"./multi_turn_convs/\")\n",
    "# Make dataset in a format that has all the information. See README for details on this format.\n",
    "control_flow_functions.convert_directory_and_process_conversations(\"./multi_turn_convs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./processed_master_list.json\") as f:\n",
    "    first = f.read()\n",
    "    data = json.loads(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For curiosity's sake, you can find out how many lines of dialogue you generated\n",
    "def filter_and_flatten(lst):\n",
    "    # Initialize an empty list to hold the flattened elements\n",
    "    flat_list = []\n",
    "\n",
    "    # Loop through each sublist in the main list\n",
    "    for sublst in lst:\n",
    "        # Check if the first element of the sublist is itself a list (subsublist1)\n",
    "        if isinstance(sublst[0], list):\n",
    "            # Extend the flat_list with the elements from subsublist1\n",
    "            flat_list.extend(sublst[0])\n",
    "\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "len(filter_and_flatten(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4976e0179d97dd6d59b1329a76e601e17b789c2571b41c8b57f5fd69821c0dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
