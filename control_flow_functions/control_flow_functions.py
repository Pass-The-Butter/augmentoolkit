import os
import asyncio
import json
import re
from tqdm import tqdm
import nltk
from nltk.tokenize import sent_tokenize
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
from collections import Counter
import tqdm.asyncio


# Graphing code generated by GPT-4. May be suboptimal/ugly.
def filter_and_graph(tuples):
    # Count the occurrences of None and non-None for each source text
    source_counts = Counter()
    for paragraph, source in tuples:
        if paragraph is None:
            source_counts[source] = source_counts.get(source, [0, 0])
            source_counts[source][0] += 1
        else:
            source_counts[source] = source_counts.get(source, [0, 0])
            source_counts[source][1] += 1

    # Prepare data for the graph
    labels = list(source_counts.keys())
    none_counts = [source_counts[source][0] for source in labels]
    non_none_counts = [source_counts[source][1] for source in labels]

    # Plotting the graph
    x = range(len(labels))
    plt.bar(x, none_counts, width=0.4, label="Not suitable", align="center")
    plt.bar(x, non_none_counts, width=0.4, label="Valid Paragraphs", align="edge")
    plt.xlabel("Source Text")
    plt.ylabel("Number of Paragraphs")
    plt.title("Paragraphs Suitable for Questions by Source Text")
    plt.xticks(x, labels, rotation="vertical")
    plt.legend()
    plt.tight_layout()
    plt.show()

    # Filter out tuples with None and return the new list
    filtered_list = [t for t in tuples if t[0] is not None]
    return filtered_list

async def determine_worthy(idx,p, judged_worthy_for_questions, engine_wrapper, output_dir, judge_paragraph):
    # for idx, p in tqdm(enumerate(paragraphs_processed[:10])):
    file_name = f"{idx}.json"
    file_path = os.path.join(output_dir, file_name)
    # Check if the judgement for this paragraph already exists
    if os.path.isfile(file_path):
        with open(file_path, "r") as file:
            data = json.load(file)
            print("LOADING: ", data)
        if isinstance(data, str):
            judged_worthy_for_questions.append((None, data[7:]))
        else:
            judged_worthy_for_questions.append((data["paragraph"], data["metadata"]))
    else:
        judgement = await judge_paragraph(p, engine_wrapper)
        judged_worthy_for_questions.append(judgement)

        # Prepare the data to be written to the file
        if judgement[0] is not None:
            # The paragraph passed the judgement
            data_to_write = {"paragraph": judgement[0], "metadata": judgement[1]}
        else:
            # The paragraph did not pass the judgement
            data_to_write = f"failed|{judgement[1]}"

        # Write the judgement to a unique file as JSON
        with open(file_path, "w") as file:
            json.dump(data_to_write, file)

        # Debug messages
        try:
            if judgement[0] is not None:
                print(f"DEBUG model decided that index {idx} was suitable")
            else:
                print(f"DEBUG model decided that index {idx} was not suitable")
        except:
            print(f"DEBUG max retries exceeded for index {idx}")

async def filter_all_questions(paragraphs_processed, judged_worthy_for_questions, engine_wrapper, output_dir, judge_paragraph):
    tasks = [determine_worthy(idx, p,judged_worthy_for_questions, engine_wrapper, output_dir, judge_paragraph) for idx, p in enumerate(paragraphs_processed[:20])]
    for future in tqdm.asyncio.tqdm.as_completed(tasks):
            await future



def sentence_chunking_algorithm(file_path, tokenizer, max_token_length=400):
    """
    This function takes a plaintext file and chunks it into sentences.

    :param file_path: Path to the plaintext file
    :param tokenizer: SentencePiece tokenizer
    :param max_token_length: The maximum token length for a chunk of sentences
    :return: List of sentence chunks with source text information
    """
    sentence_chunks_with_source = []
    current_chunk = []
    token_count = 0
    source_name = file_path.replace(".txt", "")

    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # Remove Gutenberg header and footer
    content = re.sub(
        r"^.*?START OF (THIS|THE) PROJECT GUTENBERG EBOOK.*$\n",
        "",
        content,
        flags=re.MULTILINE,
    )
    content = re.sub(
        r"^.*?END OF (THIS|THE) PROJECT GUTENBERG EBOOK.*$\n",
        "",
        content,
        flags=re.MULTILINE,
    )

    sentences = sent_tokenize(content)

    for sentence in tqdm(sentences, desc=f"Processing {file_path}"):
        sentence_token_count = len(tokenizer.encode(sentence))

        if token_count + sentence_token_count <= max_token_length:
            current_chunk.append(sentence)
            token_count += sentence_token_count
        else:
            sentence_chunks_with_source.append((" ".join(current_chunk), source_name))
            current_chunk = [sentence]
            token_count = sentence_token_count

    # Add the last chunk if it exists
    if current_chunk:
        sentence_chunks_with_source.append((" ".join(current_chunk), source_name))

    return sentence_chunks_with_source




def fix_text(to_replace_arr, text):
    for startup in to_replace_arr:
        text = text.replace(startup[0], startup[1])
    return text


async def ensure_multiple_answers_are_same(
    info, conv, engine_wrapper
):  # why is this a whole separate function? Once upon a time, LLMs were used in validation here, too. But programmatic validation SEEMS to catch the common problems. This is here so that I can add it back in if I have to.
    """Loop to ensure that the answer is consistent in the conversation and in the tuple."""
    retries = 0
    c = conv
    while retries < 2:  # try twice, since multiturn is an expensive operation
        if call_all_processors(c[0], info[0]):  # if programmatic validation passes
            return c

        retries += 1
        if retries >= 2:
            return None
        # If we're here, majority of relevance checks failed
        print("----------------\n\n\n\nRETRYING!!!!\n\n\n\n----------------")
        # Broken info is 1) rare and 2) handled by the retry limit. We don't want to waste compute on regenerating info as they take time.
        retry = await make_multiturn_conversation(info, engine_wrapper)
        if retry is not None:  # Note: retry CANNOT actually be None
            c = retry
        else:
            # If we failed to generate a retry, don't waste compute
            return None

    return None



async def make_multiturn_conversation(info, engine_wrapper):
    conv, conv_output = await multi_turn_conversation(
        info[0], info[1], info[2], info[3], engine_wrapper, assistant_mode=ASSISTANT_MODE
    )
    write_output_to_file(conv_output, "./multiturn_conversation_generations", info[4])

    return conv

